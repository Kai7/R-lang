---
title: 'Lecture 5: Monte Carlo simulations'
output: html_document
date: "2016/10/12"
---

# Parametric simulations

## RMD_example 5.1: Monte Carlo simulation for the CLT 

First read the control population:

```{r}
library(dplyr)
dat <- read.csv("mice_pheno.csv")
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>% select(Bodyweight) %>% unlist
```

Then, build a function that automatically generates a t-statistic under the null hypothesis for any sample size of `n`

```{r}
ttestgenerator <- function(n) {
  # sample cases from controls because we are modelling the null
  cases <- sample(controlPopulation,n)
  controls <- sample(controlPopulation,n)
  tstat <- (mean(cases)-mean(controls)) / sqrt( var(cases)/n + var(controls)/n )
return(tstat)
}
```

Simulate 1000 t-statistics

```{r}
ttests <- replicate(1000, ttestgenerator(10))
```

With 1,000 Monte Carlo simulated occurrences of this random variable (t-statistic), we can now get a glimpse of its distribution:

```{r}
hist(ttests)
```

So is the distribution of this t-statistic well approximated by the normal distribution? We can use the quantile-quantile (q-q) plots: if points fall on the identity line, it means the approximation is a good one.

```{r}
qqnorm(ttests)
abline(0,1)
```

This looks like a very good approximation. For this particular population, a sample size of 10 was large enough to use the CLT approximation. How about 3?

```{r}
ttests <- replicate(1000, ttestgenerator(3))
qqnorm(ttests)
abline(0,1)
```

Now we see that the large quantiles, referred to by statisticians as the tails, are larger than expected (below the line on the left side of the plot and above the line on the right side of the plot). In the previous module, we explained that when the sample size is not large enough and the population values follow a normal distribution, then the t-distribution is a better approximation. Our simulation results seem to confirm this:

```{r}
ps <- (seq(0,999)+0.5)/1000
qqplot(qt(ps,df=2*3-2),ttests,xlim=c(-6,6),ylim=c(-6,6))
abline(0,1)
```

The t-distribution is a much better approximation in this case, but it is still not perfect. This is due to the fact that the original data is not that well approximated by the normal distribution.

```{r}
qqnorm(controlPopulation)
qqline(controlPopulation)
```

## RMD_example 5.2: Parametric simulations for the observations

For a parametric simulation, we take parameters estimated from the real data (here the mean and the standard deviation), and plug these into a model (here the normal distribution). 

For the case of weights, we could use our knowledge that mice typically weigh 24 ounces with a SD of about 3.5 ounces, and that the distribution is approximately normal. We can then generate the (control) population using `rnorm`, and modify the `ttestgenerator` function as follows:

```{r}
ttestgenerator <- function(n, mean=24, sd=3.5) {
  cases <- rnorm(n,mean,sd)
  controls <- rnorm(n,mean,sd)
  tstat <- (mean(cases)-mean(controls)) / sqrt( var(cases)/n + var(controls)/n )
return(tstat)
}
```

Do 1000 simulations for sample size 3, and obtain its distribution:

```{r}
n <- 3
ttests <- replicate(1000, ttestgenerator(n))
hist(ttests)

qqnorm(ttests)
abline(0,1)

ps <- (seq(0,999)+0.5)/1000
qqplot(qt(ps,df=2*n-2),ttests,xlim=c(-6,6),ylim=c(-6,6))
abline(0,1)
```

# Nonparametric simulations

## RMD_example 5.3: Bootstrap

Suppose we have a situation in which none of the standard mathematical statistical approximations apply. We have computed a summary statistic, such as the sample standard deviation $s_{X}$, but do not have a useful approximation, such as that provided by the CLT. In practice, we do not have access to all values in the population so we canâ€™t perform a simulation as done above. Bootstrap can be useful in these scenarios.

We are back to the scenario where we only have 12 measurements for the control group.

```{r}
dat <- read.csv("femaleMiceWeights.csv")

library(dplyr)
control <- filter(dat,Diet=="chow") %>% select(Bodyweight) %>% unlist
sd(control)
```

We use the sample standard deviation $s_{X}$ to estimate the population standard deviation $\sigma_{X}$. To know how good this estimation is, we like to obtain standard error $\sqrt{{\rm var}(s_{X})}$ and the 95% CI for $\sigma_{X}$. Using bootstrap, these are

```{r}
B <- 1000
n <- 12
M <- numeric(B)
for (b in 1:B) {
  i <- sample(1:n, size = n, replace = TRUE)
  M[b] <- sd(control[i])
}
sd(M)
quantile(M, c(0.025, 0.975))
```

## RMD_example 5.4: Permutation tests

```{r}
# read observed data
dat <- read.csv("femaleMiceWeights.csv")

library(dplyr)
control <- filter(dat,Diet=="chow") %>% select(Bodyweight) %>% unlist
treatment <- filter(dat,Diet=="hf") %>% select(Bodyweight) %>% unlist
obsdiff <- mean(treatment)-mean(control)

# perform permutation 1,000 times
N <- 12
avgdiff <- replicate(1000, {
  all <- sample(c(control,treatment))
  newcontrols <- all[1:N]
  newtreatments <- all[(N+1):(2*N)]
  return(mean(newtreatments) - mean(newcontrols))
})

hist(avgdiff)
abline(v=obsdiff, col="red", lwd=2)
```

How many of the null means are bigger than the observed value? That proportion would be the p-value for the null. We add a 1 to the numerator and denominator to account for misestimation of the p-value.

```{r}
(sum(abs(avgdiff) > abs(obsdiff)) + 1) / (length(avgdiff) + 1)
```

